# 难点和改进方案



## 非正常状态时的数据访问

缓存服务器在迁移数据或者恢复数据的时候，怎么样不影响客户端的读和写

1、给master_server设置状态位，Normal、Moving、Recovering，保留更新之前的一致性哈希，以及新的一致性哈希，cache_server中的数据设置dirty标志位

​	Normal：正常读写，cache_server端实现读共享，写互斥

​	Moving：返回三个地址，原ip、新ip、新备份ip

​					客户端先去原ip上找，找到数据为dirty或者找不到就去新的ip上找，写数据直接写到新ip上

​	Recovering：返回三个地址，原备份ip、新ip、新备份ip

​					客户端先去原备份ip上读，读不到或者读到dirty数据，说明已恢复，就去新ip上，写数据直接写到新ip

**改进方案：**

1）每次客户端请求直接把所有在线的cache_server的地址都返回，在客户端重建一份一致性哈希表，这样可以降低master_server的压力。当客户端读不到数据、读到dirty数据，由cache_server通知客户端，你的哈希表过期了，客户端重新向master获取新的数据分布。

2）每次setVal的时候，cache_server都会执行检查，如果发现数据不属于自己就通知客户端哈希表过期了



## 心跳机制

cache_server和master_server如何知道对方掉线了

1、目前采用的方法是利用TCP的长连接，利用操作系统的机制，即断开连接会主动发送FIN包，但是这样是不安全的，如果是操作系统崩溃了、突然停电了、网络断了等原因，就需要等到TCP保活计时器超时才能发现，需要在应用层实现更可靠的检测机制

**改进方案：**

1）server定期向client发送心跳包，如果客户端连续几次未回复，server主动断开连接

2）client定期向server发送心跳包，server收到心跳包后启动一个定时器，如果超时未收到客户端的心跳，断开连接









## 通信方式

​	目前采用的是请求——响应的模式，客户端与master_server之间采用UDP连接，与cache_server之间用TCP连接，cache_server之间以及和master也采用TCP连接

​	对master而言没有什么问题，毕竟cache_server是有限的，不会有太多的TCP连接，但是对于cache_server而言，既要接受用户的访问，也要执行数据的移动、恢复、备份，这些都是TCP连接，比较消耗资源和性能，网络框架采用的epoll多路复用，当用户量非常大的时候，移动、恢复、备份的请求可能被淹没掉了，毕竟线程池的请求队列是有限的

改进方案：

1）数据的备份操作由客户端来进行，同时访问主备服务器

2）基于UDP，在应用层实现可靠的通信机制，即请求和应答，如果请求没有被应答，就再次请求





